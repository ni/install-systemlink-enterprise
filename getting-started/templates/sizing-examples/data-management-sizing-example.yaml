# Example configuration for tuning sizing configuration for services related to data management.

## The data frame service stores and indexes tabular data.
dataframeservice:
  # Data frame core service resources
  resources:
    requests:
      memory: 4096Mi
      cpu: 250m
    limits:
      memory: 4096Mi
  autoscaling:
    minReplicas: 2
    maxReplicas: 10
    targetCPUUtilizationPercentage: 60
    targetMemoryUtilizationPercentage: 70

  ## Data frame nessie container resources
  nessie:
    resources:
      requests:
        memory: 180Mi
        cpu: 100m
      limits:
        memory: 180Mi
    autoscaling:
      minReplicas: 2
      maxReplicas: 10
      targetCPUUtilizationPercentage: 60
      targetMemoryUtilizationPercentage: 80

  # Limits the body size for requests. The ingress may also impose a request body size
  # limit, which should be set to the same value.
  # When modifying ingestion.s3Backend.rowGroupSize, make a proportional change to this value.
  # Accepts units in "MiB" (Mebibytes, 1024 KiB) or in "MB" (Megabytes, 1000 KB)
  requestBodySizeLimit: 256MiB

  ## Configuration for DataFrame writes
  ingestion:
    # Configuration for ingestion requests in the Apache Arrow format.
    arrow:
      # The maximum number of bytes the server will allocate to hold each record batch during
      # an ingestion request. This value must be greater than zero. Larger values require
      # additional pod memory per request.
      # For non-compressed requests, the service compares this value to the size of the record
      # batch in memory. For compressed requests, the service compares this value to the total
      # size of each compressed value buffer and the uncompressed record batch in memory.
      maxRecordBatchSize: 32MiB

    # Configuration for the pool of streams used to upload table data to storage such as S3 or
    # Azure Blob Storage.
    streamPool:
      # Size of each buffer in the stream pool. This value must be greater than zero.
      # When "storage.type" is "s3", this value must be greater or equal to "storage.s3.minimumPartSize".
      # The product of this value and "maximumPooledStreams", must be less than the memory requested for
      # the service in "resources.requests.memory".
      bufferSize: 15MiB
      # Maximum number of streams that will be pooled.
      # The recommendation is to provide the same number of pool streams as the limit of requests that can be processed
      # in "rateLimits.ingestion.requestsLimit".
      # The product of this value and "bufferSize", must be less than the memory requested for
      # the service in "resources.requests.memory".
      # WARNING: Setting this value 0 would leave the pool unbounded, which could cause high memory usage.
      maximumPooledStreams: 20
    # Configuration for the S3 ingestion backend.
    s3Backend:
      # Approximate number of values to buffer into memory for writing to a parquet row group.
      # Values that are too small may produce parquet files that can't be queried. Larger
      # values require additional memory.
      # When adjusting this value, adjust requestBodySizeLimit proportionally.
      rowGroupSize: "1000000"
      ## @param dataframeservice.ingestion.s3Backend.appendableTableLimit The number of distinct, appendable tables
      ## that can use the S3 ingestion backend before the service blocks table creation. To stay under this limit,
      ## set 'endOfData: true' on tables that no longer need appending. For more information, refer to
      ## the following website: ni.com/r/setendofdata.
      appendableTableLimit: "100000"

    ## @param dataframeservice.ingestion.maxColumnCount The maximum number of columns that a data table may be created with.
    ## Attempting to create a table with more columns than the specified limit will result in a 400 Bad Request response.
    maxColumnCount: 2500

    ## @param dataframeservice.ingestion.maxRowCount The maximum number of rows that can be written to a single table across
    ## all requests. Once a table's row count reaches this limit, the table is effectively read-only.
    ## Query performance for tables with a large number of rows is highly dependent upon the
    ## number of columns, the resources available to Dremio, and the number of executors.
    ## The default is 1 billion rows. This value must be greater than zero and less than 2147483648.
    maxRowCount: "1000000000"

  rateLimits:
    # Configures the global rate limiter for all requests
    global:
      # Sets the number of tokens to replenish per second
      tokensPerSecond: 750
      # Sets the maximum number of tokens that the system can accumulate
      tokenLimit: 200
      # Sets the number of requests that may queue while tokens are unavailable
      queueLimit: 0
    ingestion:
      # Number of concurrent requests that a single pod can serve for ingesting data.
      # Subsequent requests will be put in a queue.
      requestsLimit: 20
      # Size of the queue for concurrent requests. If a request arrives to a pod with a full queue,
      # the pod will return a 429 Error code.
      queueSize: 0
    # Configures the per-user rate limit for metadata queries, including for the following routes:
    #   - GET /v1/tables
    #   - GET /v1/tables/{id}
    #   - POST /v1/query-tables
    queryMetadata:
      # Sets the number of tokens to replenish per second
      tokensPerSecond: 5
      # Sets the maximum number of tokens that the system can accumulate
      tokenLimit: 5
      # Sets the number of requests that may queue while tokens are unavailable
      queueLimit: 1
    # Configures the per-user rate limit for row data queries, including for the following routes:
    #   - GET /v1/table/{id}/data
    #   - POST /v1/tables/{id}/query-data
    #   - POST /v1/tables/{id}/query-decimated-data
    #   - POST /v1/tables/{id}/export-data
    queryRowData:
      # Sets the number of tokens to replenish per second
      tokensPerSecond: 5
      # Sets the maximum number of tokens that the system can accumulate
      tokenLimit: 5
      # Sets the number of requests that may queue while tokens are unavailable
      queueLimit: 1

  ## Dremio configuration
  sldremio:
    ## Resource requests for the coordinator. For information on resource requirements,
    ## refer to the Dremio documentation: https://docs.dremio.com/current/deploy-dremio/other-options/standalone/system-requirements#server-or-instance-hardware
    coordinator:
      ## @param dataframeservice.sldremio.coordinator.cpu CPU allocated to each coordinator, expressed in CPU cores.
      cpu: 3
      ## @param dataframeservice.sldremio.coordinator.memory Memory allocated to each coordinator, expressed in MB.
      memory: 16384
      ## @param dataframeservice.sldremio.coordinator.volumeSize Coordinator data volume size (applies to the master coordinator only).
      volumeSize: 256Gi
    ## Resource requests for the executor. For information on resource requirements,
    ## refer to the Dremio documentation: https://docs.dremio.com/current/deploy-dremio/other-options/standalone/system-requirements#server-or-instance-hardware
    executor:
      ## @param dataframeservice.sldremio.executor.count Number of executors.
      count: 3
      ## @param dataframeservice.sldremio.executor.cpu CPU allocated to each executor, expressed in CPU cores.
      cpu: 15
      ## @param dataframeservice.sldremio.executor.memory Memory allocated to each executor, expressed in MB.
      memory: 73728
      ## @param dataframeservice.sldremio.executor.volumeSize Executor volume size.
      volumeSize: 256Gi
      ## Override configuration for the Iceberg executors. This will create a separate group of Iceberg executors
      ## in parallel with the standard executors. Standard executors process queries, while iceberg executors process writes.
      engineOverride:
        iceberg:
          ## @param dataframeservice.sldremio.executor.engineOverride.iceberg.count Number of executors.
          count: 4
          ## @param dataframeservice.sldremio.executor.engineOverride.iceberg.cpu CPU allocated to each executor, expressed in CPU cores.
          cpu: 9
          ## @param dataframeservice.sldremio.executor.engineOverride.iceberg.memory Memory allocated to each executor, expressed in MB.
          memory: 32768
          ## @param dataframeservice.sldremio.executor.engineOverride.iceberg.heapMemoryOverride Configures the heap memory pool for the executor.
          heapMemoryOverride: 20000
          ## @param dataframeservice.sldremio.executor.engineOverride.iceberg.directMemoryOverride Configures the direct memory pool for the executor.
          directMemoryOverride: 10000

  ## Workload configuration
  workloadManagement:
    # COPY INTO jobs are targeted to this queue. Increase the concurrency limit
    # to make newly ingested data available for query faster. This may require
    # adding more resources to the engine the query is targeting.
    writeQueue:
      ## @param dataframeservice.workloadManagement.writeQueue.concurrencyLimit Number of parallel jobs processed from the queue.
      concurrencyLimit: 20
    # Compaction jobs are targeted to this queue. Increase the concurrency limit
    # to make newly ingested data available for query faster. This may require
    # adding more resources to the engine the query is targeting.
    optimizeQueue:
      ## @param dataframeservice.workloadManagement.optimizeQueue.concurrencyLimit Number of parallel jobs processed from the queue.
      concurrencyLimit: 15
    # Dremio targets high-cost queries, such as decimation queries that scan tables with
    # more than a million rows, to this queue.
    highCostUserQueriesQueue:
      ## @param dataframeservice.workloadManagement.highCostUserQueriesQueue.concurrencyLimit This parameter increases the
      ## concurrency limit. This limit supports SystemLink by allowing the creation of more concurrent queries to handle
      ## large tables. When increasing this limit, monitor executor resource usage and increase if necessary.
      concurrencyLimit: 10
    # Dremio targets low-cost queries to this queue.
    lowCostUserQueriesQueue:
      ## @param dataframeservice.workloadManagement.lowCostUserQueriesQueue.concurrencyLimit This parameter increases the
      ## concurrency limit. This limit supports SystemLink by allowing the creation of more concurrent queries to handle
      ## small tables. When increasing this limit, monitor executor resource usage and increase if necessary.
      concurrencyLimit: 10
    # Dremio targets low-cost queries to this queue.
      concurrencyLimit: 100

  ## @param dataframeservice.requestBodySizeLimit Limits the body size for requests. The ingress may also impose a request body
  ## size limit, which should be set to the same value.
  ## Accepts units in "MiB" (Mebibytes, 1024 KiB) or in "MB" (Megabytes, 1000 KB)
  requestBodySizeLimit: 256MiB

  ## This shows an example of configuring an nginx ingress to support large uploads.
  ingress:
    annotations:
      # Increase request size limit to allow large uploads.
      nginx.ingress.kubernetes.io/proxy-body-size: 256m
      # Increase HTTP timeouts to allow for long-running synchronous read requests.
      nginx.ingress.kubernetes.io/proxy-connect-timeout: "600"
      nginx.ingress.kubernetes.io/proxy-send-timeout: "600"
      nginx.ingress.kubernetes.io/proxy-read-timeout: "600"

## The file ingestion service stores files.
fileingestion:
  resources:
    requests:
      memory: 1Gi
      cpu: 500m
    limits:
      memory: 1Gi
  autoscaling:
    minReplicas: 2
    maxReplicas: 10
    targetCPUUtilizationPercentage: 60
    targetMemoryUtilizationPercentage: 70

  ## @param fileingestion.uploadLimitGB Limit of the maximum accepted size of uploaded files, expressed in GB.
  uploadLimitGB: 10

  ## This shows an example of configuring an nginx ingress to support large uploads.
  ingress:
    annotations:
      nginx.ingress.kubernetes.io/proxy-body-size: 2000m
      nginx.ingress.kubernetes.io/proxy-request-buffering: "off"
      nginx.ingress.kubernetes.io/proxy-buffering: "off"

## The specification management service stores specification documents.
specificationmanagement:
  ## @param specificationmanagement.enabled Removes this service from the deployment if false. Disable this
  ## service if it is not required.
  enabled: true

  resources:
    limits:
      memory: "512Mi"
    requests:
      cpu: "250m"
      memory: "512Mi"
  autoscaling:
    minReplicas: 2
    maxReplicas: 10
    targetCPUUtilizationPercentage: 60
    targetMemoryUtilizationPercentage: 80

    ## @param specificationmanagement.maxSpecificationCreationCountPerRequest Maximum number of specifications that can be created per request
    maxSpecificationCreationCountPerRequest: 1000
    ## @param specificationmanagement.maxSpecificationDeletionCountPerRequest Maximum number of specifications that can be deleted per request
    maxSpecificationDeletionCountPerRequest: 10000
    ## @param specificationmanagement.maxSpecificationQueryCountPerRequest Maximum number of specifications that can be queried per request
    maxSpecificationQueryCountPerRequest: 10000
    ## @param specificationmanagement.maxSpecificationUpdateCountPerRequest Maximum number of specifications that can be updated per request
    maxSpecificationUpdateCountPerRequest: 100

## The test monitor service stores test results and products.
testmonitorservice:
  resources:
    requests:
      memory: "2Gi"
      cpu: "250m"
    limits:
      memory: "2Gi"
  autoscaling:
    minReplicas: 2
    maxReplicas: 10
    targetCPUUtilizationPercentage: 60
    targetMemoryUtilizationPercentage: 80

    ## @param testmonitorservice.requestMaximumBatchLimit The maximum number of steps that may be written in a single request.
    requestMaximumBatchLimit: 10000
    ## @param testmonitorservice.requestTakeLimit Set to configure the maximum take value for paginated API requests.
    ## This is used to avoid OutOfMemoryExceptions on large requests and to avoid potential DoS attacks.
    ## Set to -1 to disable the limit.
    requestTakeLimit: 100000

## The work item service stores work items and is used to schedule and execute work items.
workitem:
  resources:
    limits:
      memory: "512Mi"
    requests:
      cpu: "250m"
      memory: "512Mi"
  autoscaling:
    minReplicas: 2
    maxReplicas: 10
    targetCPUUtilizationPercentage: 60
    targetMemoryUtilizationPercentage: 80

  ## @param workitem.maximumWorkOrdersPerCreateRequest Maximum number of work orders per create work orders request.
  maximumWorkOrdersPerCreateRequest: 1000
  ## @param workitem.maximumTakePerQueryWorkOrdersRequest Maximum query take per query work orders request.
  maximumTakePerQueryWorkOrdersRequest: 1000
  ## @param workitem.maximumWorkOrdersPerUpdateRequest Maximum number of work orders per update work orders request.
  maximumWorkOrdersPerUpdateRequest: 1000
  ## @param workitem.maximumWorkOrdersPerDeleteRequest Maximum number of work orders per delete work orders request.
  maximumWorkOrdersPerDeleteRequest: 1000
  ## @param workitem.maximumWorkItemsPerCreateRequest Maximum number of work items per create work items request.
  maximumWorkItemsPerCreateRequest: 1000
  ## @param workitem.maximumTakePerQueryWorkItemsRequest Maximum query take per query work items request.
  maximumTakePerQueryWorkItemsRequest: 1000
  ## @param workitem.maximumWorkItemsPerUpdateRequest Maximum number of work items per update work items request.
  maximumWorkItemsPerUpdateRequest: 1000
  ## @param workitem.maximumWorkItemsPerDeleteRequest Maximum number of work items per delete work items request.
  maximumWorkItemsPerDeleteRequest: 1000
  ## @param workitem.maximumWorkItemsPerScheduleRequest Maximum number of work items per schedule work items request.
  maximumWorkItemsPerScheduleRequest: 1000
  ## @param workitem.maximumWorkItemTemplatesPerCreateRequest Maximum number of work item templates per create work
  ## item templates request.
  maximumWorkItemTemplatesPerCreateRequest: 1000
  ## @param workitem.maximumTakePerQueryWorkItemTemplatesRequest Maximum query take per query work item templates request.
  maximumTakePerQueryWorkItemTemplatesRequest: 1000
  ## @param workitem.maximumWorkItemTemplatesPerDeleteRequest Maximum number of work item templates per delete work item
  ## templates request.
  maximumWorkItemTemplatesPerDeleteRequest: 1000
  ## @param workitem.maximumWorkItemTemplatesPerUpdateRequest Maximum number of work item templates per update work item
  ## templates request.
  maximumWorkItemTemplatesPerUpdateRequest: 1000
