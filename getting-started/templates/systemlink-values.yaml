## Default values for systemlink.
## This is a YAML-formatted file.
## Declare override values for variables.

## Global variables expected to be inherited from parent helm charts which apply to the entire application.
##
global:
  ## Host names for the cluster's UI ingress controller.
  # <ATTENTION> - Set this to the DNS address where the SystemLink UI will be hosted.
  ##
  hosts: &uiHosts
    - &primaryUIHost "systemlink.example.com"
  ## Host names for the cluster's API ingress controller.
  # <ATTENTION> - Set this to the DNS address to be used for communication between client systems and the SystemLink API.
  ##
  apiHosts: &apiHosts
    - &primaryApiHost "systemlink-api.example.com"
  ## Defines the secret required to pull containers hosted on a private image repository.
  ##
  niImagePullSecret: &niPullSecret "niartifacts-secret"
  ## Defines secrets required if containers are hosted on a private image repository.
  ##
  imagePullSecrets: &pullSecrets [*niPullSecret]
  ## Overrides the default image registry.
  # <ATTENTION> - Use this override if mirroring the SystemLink container registry.
  ##
  imageRegistry: &imageRegistryRef "niedge01.jfrog.io/ni-docker"
  ##
  ## Defines a secret containing the credentials for a repo hosting miscellaneous artifacts
  ##
  genericArtifactPullSecret: "niartifacts-generic"
  ## Ingress settings that apply globally.
  # <ATTENTION> - Use the following section to apply annotation-based configuration to all Systemlink ingresses.
  #    Configuration is specific to the ingress controller, and the defaults will be sufficient for many deployments.
  ##
  ingress:
    ## Ingress settings that apply to the apiHosts endpoints.
    ##
    api:
      ## Annotations for the ingress.
      ##
      annotations: {}
    ## Ingress settings that apply to the hosts endpoints.
    ##
    ui:
      ## @param global.ingress.ui.annotations Annotations for the ingress.
      ##
      annotations: {}
  ## Disable secret deployment if you want to manually manage secrets on the cluster.
  ## WARNING: Changing this value from true to false during an upgrade will delete existing secrets.
  # <ATTENTION> - Set to false if you do not want to manage secrets as part of the Helm installation.
  ##
  deploySecrets: true
  ## Customize telemetry data collection.
  ##
  telemetry:
    ## Specify whether to collect telemetry data.
    ##
    enabled: false
    ## Url target of OpenTelemetry exporter. Corresponds to OTEL_EXPORTER_OTLP_ENDPOINT.
    ## https://opentelemetry.io/docs/concepts/sdk-configuration/otlp-exporter-configuration/#otel_exporter_otlp_endpoint
    ##
    openTelemetryExporterOtlpEndpoint: ""
    ## Specify whether to include user IDs in traces.
    ## Turned off by default per recommendation of the OpenTelemetry Specification.
    ## https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/trace/semantic_conventions/span-general.md#general-identity-attributes
    ##
    includeUserInfo: false

## Core configuration for the RabbitMQ message broker
##
rabbitmq:
  ## Policy used when starting pods in the stateful set.
  # <ATTENTION> - RabbitMQ pods must start in reverse shutdown order.
  #               To restore the cluster to a healthy state after an unexpected shutdown, temporarily set
  #               podManagementPolicy to "Parallel" and forceBoot to true.
  #               https://github.com/bitnami/charts/tree/main/bitnami/rabbitmq#recover-the-cluster-from-complete-shutdown
  ##
  podManagementPolicy: "OrderedReady"
  # podManagementPolicy: "Parallel"
  clustering:
    ## Force cluster to boot after an unexpected shutdown (in an unexpected order).
    ##
    forceBoot: false

## Core configuration for the SystemLink web application.
##
webserver:
  ## Values used to configure OpenID Connect providers.
  # <ATTENTION> - Configure your OpenID Connect provider here, as well as the claims to make available to the SystemLink application.
  #    Consult your provider's documentation regarding exposing scopes to clients.
  ##
  oidc:
    ## Required - Secret name that holds client ID, client secret, and JWKs.
    ##
    secretName: "oidc-secret"
    ## Required - Include either issuer URL (for discovery) or the provider configuration as JSON wrapped in single quotes.
    ##
    issuer: "https://oidc.example.com/"
    # provider: '<provider-config-json>'
    ## Optional - Claim to use for user ID.
    ##
    # userIDClaim: "email"
    ## Optional - Claim to use for user name.
    ##
    # usernameClaim: "name"
    ## Optional - Scopes requested from the provider.
    ## At minimum, "openid" scope is required, "email" and "profile" are required to populate user preferences
    ##
    scope: "openid email profile"
    # Optional - Enable the user-info end-point to return the user's claims
    ##
    # enableUserInfo: "false"
  ## If access to the OpenID Connect provider requires a proxy server, provide proxy service configuration here.
  # <ATTENTION> - Configure the proxy server for OpenID connect access here.
  ##
  proxy:
    ## The host and port of the proxy, no schema
    ##
    # authority: "example.com:8080"
    ## Optional - The name of the secret that has the proxy credentials
    ##
    # secretName: "webserver-proxy-credentials"
    ## Optional - The key for the username in the credentials secret
    ##
    userSecretKey: "username"
    ## Optional - The key for the password in the credentials secret
    ##
    passwordSecretKey: "password"

## Defines an initial administrator role mapping, allowing an initial user or users to access the system.
## At least one user must be defined during installation to allow access to the application. The initial user(s) can then provision
## access for further users. By default, users are only created on install, not during upgrades.
##
userservicesetup:
  initialAdministratorMapping:
    ## Overrides default behavior and creates or updates an administrator user mapping on upgrade.
    # <ATTENTION> - If true, the configured administrator will be reconfigured on upgrade.
    ##
    createOnUpgrade: false
    ## The mapping type used to define the administrator user.
    ##
    mappingType: "oidc-claim"
    ## The property used to match mappingValue.
    ##
    mappingKey: "email"
    ## The property value used to identify admin users. This value is required when creating a new user mapping.
    ## To avoid unwanted information disclosure, it is recommended to only set this value when creating a new mapping.
    # <ATTENTION> - Choose an initial system administrator here. You must configure an administrator to enable access
    #   to the SystemLink application. This example shows configuring an administrator by email address, but any
    #   OpenID Connect claim can be used to define this mapping.
    ##
    mappingValue: "user@example.com"

## Shared database configuration.
##
database:
  ## If using an externally managed PostgresSQL database, this value can be used to define a public TLS certificate used
  ## to authenticate with that database. The following argument can be used to load a certificate file from disk as part of a
  ## Helm install/upgrade command:
  ##       --set-file database.postgresCertificate my-cert.pem
  ##
  postgresCertificate: ""
  ## Name that will be used for this certificate when mounted on disk.
  ##
  postgresCertificateFileName: &postgresCertificateFileName "postgres-tls-certificate.pem"
  ## Name of the ConfigMap used to deploy the certificate.
  ##
  postgresCertificateConfigMapName: &postgresCertificateConfigMap "postgres-tls-certificate"

## MinIO block storage configuration.
##
minio:
  ## Set to false to disable deployment of the minio service. If minio is not deployed, an alternative S3 provider
  ## must be configured. Refer to the fileingestion.s3, nbexecservice.argo.s3, and dataframeservice.s3 sections
  ## to configure an S3 provider.
  # <ATTENTION> - Set to false if not using minio for S3 storage.
  ##
  enabled: true

  ## String to partially override common.names.fullname template (will maintain the release name)
  ##
  nameOverride: &minioServiceName "minio"

  ## Setup user credentials.
  ##
  auth:
    existingSecret: "minio-credentials"

  ## Port configuration
  containerPorts:
    ##  MinIO container port to open for MinIO API
    ##
    api: &minioPort 9000

  ## Storage configuration.
  persistence:
    ## PVC Storage Request for MinIO data volume
    # <ATTENTION> - Adjust this size to match the needs of your application. MinIO will be used to store uploaded files and data frames.
    ##
    size: 50Gi

  ## Configure the ingress resource that allows you to access the MinIO UI.
  ## ref: https://kubernetes.io/docs/user-guide/ingress/
  ##
  ingress:
    ## Enable ingress controller resource.
    # <ATTENTION> - Enable this toggle and configure a host name to allow access to the MinIO UI
    ##
    enabled: false
    ## @param ingress.hostname Default host for the ingress resource.
    ##
    hostname: "systemlink-minio.example.com"

  ## Configure the ingress resource that allows you to access the MinIO API.
  ## ref: https://kubernetes.io/docs/user-guide/ingress/
  ##
  apiIngress:
    ## Enable ingress controller resource.
    # <ATTENTION> - Enable this toggle and configure a host name to allow access to the MinIO API
    ##
    enabled: false
    ## Default host for the ingress resource.
    ##
    hostname: "systemlink-minio-api.example.com"
  ## Comma, semi-colon or space separated list of buckets to create at initialization (only in standalone mode).
  ##
  defaultBuckets: "systemlink-file-ingestion;systemlink-dataframe;systemlink-notebook-execution"

## Configuration for test result storage.
##
testmonitorservice:
  ## Database configuration
  ##
  database:
    ## The PostgreSQL database connection string
    ## NOTE: If specified, the `database.connectionInfo` parameters are ignored.
    ## If `database.tls.enabled` is set to `true`, the connection string must include the
    ## appropriate SSL Mode (Prefer or Require).
    # <ATTENTION> - If connecting to an external PostgresSQL database, you must configure one of
    #    the connectionString and connectionInfo sections with the details of your database.
    ##
    connectionString:
      secretName: "testmonitorservicedb-connection"
      connectionStringKey: "connection-string"
    ## The PostgreSQL database connection info.
    ## NOTE: If the `database.connectionString` parameters are specified, the `database.connectionInfo`
    ## parameters are ignored.
    ##
    # connectionInfo:
    #     ## PostgreSQL host name.
    #     ##
    #     host: ""
    #     ## PostgreSQL port.
    #     ##
    #     port: ""
    #     ## PostgreSQL database name.
    #     ##
    #     dbName: "nisystemlink"
    #     ## PostgreSQL user name.
    #     ##
    #     user: "nisystemlink"
    #     ## The name of an existing secret with PostgreSQL connection credentials.
    #     ##
    #     secretName: "testmonitorservicedb-connection"
    #     ## Password key to be retrieved from existing secret.
    #     ## NOTE: Ignored unless `database.connectionInfo.secretName` parameter is set.
    #     ##
    #     passwordKey: "password"
    ## The PostgreSQL database TLS configuration
    ##
    tls:
      ## Enable TLS communication with the PostgreSQL database.
      ## NOTE: If true, the public TLS certificate from the server (.pem or .crt)
      ## must be uploaded to the cluster as a Kubernetes ConfigMap and its name and key
      ## be set under the `database.tls.existingConfigMap` and `database.tls.certificateSubPath`
      ## parameters respectively.
      ## Use the database.postgresCertificate value to automatically upload this certificate.
      ##
      enabled: true
      ## The name of an existing ConfigMap with a TLS certificate for the database.
      ##
      existingConfigMap: *postgresCertificateConfigMap
      ## The Certificate key to be retrieved from existing ConfigMap
      ##
      certificateSubPath: *postgresCertificateFileName

## Configuration for the Grafana dashboard provider.
##
dashboardhost:
  ## Name to use for the database access secret.
  ##
  databaseSecretName: &dashboardhostdbSecret "dashboardhost-postgres-secrets"

  ## Override Grafana ingress to enable pulling in globally-defined ingress annotation.
  ## The Grafana ingress is disabled by default so no further configuration is required under the "grafana" section.
  ##
  ingress:
    ## Enable the ingress.
    ##
    enabled: true
    ## For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
    # <ATTENTION> - Configure you ingress controller class name.
    ##
    ingressClassName: "nginx"
    ## Values can be templated.
    ##
    annotations: {}
    labels: {}
    path: "/dashboardhost"
    ## pathType is only for k8s >= 1.1=.
    ##
    pathType: "Prefix"
    hosts: *uiHosts
    ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.
    ##
    extraPaths: []
    tls: []

  ## Grafana community chart configuration. See https://github.com/grafana/helm-charts/blob/main/charts/grafana/README.md
  ## for more documentation and examples for these values.
  ##
  grafana:

    ## Configure access to the Grafana container.
    ##
    image:
      ## Image pull secrets required by Grafana
      pullSecrets: *pullSecrets

    ## Node count. This must be set to 1 if Grafana is not configured to use an external database.
    ## This value is only used if autoscaling is disabled.
    ##
    replicas: 1

    ## Create HorizontalPodAutoscaler object for deployment type
    ##
    autoscaling:
      # <ATTENTION> - If not using an external database, you must disable autoscaling and use a single Grafana instance.
      enabled: true
      minReplicas: 2
      maxReplicas: 10
      metrics:
      - type: "Resource"
        resource:
          name: "cpu"
          targetAverageUtilization: 60
      - type: "Resource"
        resource:
          name: "memory"
          targetAverageUtilization: 70

    ## Use an existing secret for the admin user.
    # <ATTENTION> - Uncomment this section to use a different secret to configure the admin user.
    ##
    # admin:
    #   ## The name of an existing secret containing the admin credentials.
    #   existingSecret: "dashboardhost-login"
    #   ## The key in the existing admin secret containing the user name.
    #   userKey: admin-user
    #   ## The key in the existing admin secret containing the password.
    #   passwordKey: "admin-password"

    ## Uncomment this to provision additional datasources (the chart provisions ni-slnotebook-datasource by default)
    ## When adding additional datasources, take care to leave the the ni-slnotebook-datasource entry in place, and add
    ## new entries after it.
    # <ATTENTION> - Uncomment this to provision additional data sources (the chart provisions systemlink-notebook-datasource by default)
    ##
    # datasources:
    #   datasources.yaml:
    #     apiVersion: 1
    #     datasources:
    #     - name: "SystemLink Notebooks"
    #       type: "ni-slnotebook-datasource"
    #       access: "direct"
    #       url: /
    #       version: 1

    ## Defines additional mounts from Secrets.
    # <ATTENTION> - This configures a connection to an external PostgresSQL. Remove this section if not using an external database.
    ##
    extraSecretMounts:
      - name: *dashboardhostdbSecret
        secretName: *dashboardhostdbSecret
        defaultMode: 0440
        mountPath: "/etc/secrets/dashboardhost"
        readOnly: true

    ## Defines additional mounts from ConfigMaps.
    # <ATTENTION> - This configures a connection to an external PostgresSQL. Remove this section if not using an external database.
    ##
    extraConfigmapMounts:
      - name: *postgresCertificateConfigMap
        mountPath: "/etc/ssl/certs/dashboardhost/"
        subPath: *postgresCertificateFileName
        configMap: *postgresCertificateConfigMap
        readOnly: true

    ## This sets the GF_INSTALL_PLUGINS env variable
    ## https://grafana.com/docs/grafana/latest/installation/docker/#install-plugins-from-other-sources
    ## <ATTENTION> - To include additional Grafana plugins, uncomment this section, update the existing URLs where
    ## X.X.X.zip is the latest version in the values.yaml file, and add the URLs for any additional plugins.
    ##
    # plugins:
    #   - http://localhost:8080/ni/plugins/systemlink-notebook-datasource/X.X.X.zip;ni-slnotebook-datasource
    #   - http://localhost:8080/ni/plugins/systemlink-dataframe-datasource/X.X.X.zip;ni-sldataframe-datasource
    #   - http://localhost:8080/ni/plugins/plotly-panel/X.X.X.zip;ni-plotly-panel

    ## Customize the grafana.ini file.
    ##
    grafana.ini:
      server:
        ## Host name for the Grafana instance.
        # <ATTENTION> - The DNS address of the SystemLink application must be duplicated here.
        ##
        domain: *primaryUIHost

      ## Database configuration. See here for more documentation: https://grafana.com/docs/grafana/latest/administration/configuration/#database
      # <ATTENTION> - This configures a connection to an external PostgresSQL. Remove this section if not using an external database.
      ##
      database:
        ## Either mysql, postgres or sqlite3.
        ##
        type: "postgres"
        ## The database user (not applicable for sqlite3).
        ##
        user: $__file{/etc/secrets/dashboardhost/user}
        ## The database userâ€™s password (not applicable for sqlite3). If the password contains # or ; you have to wrap it with triple quotes. For example """#password;""".
        ##
        password: $__file{/etc/secrets/dashboardhost/password}
        ## Only applicable to MySQL or Postgres. Includes IP or host name and port or in case of Unix sockets the path to it.
        ## For example, for MySQL running on the same host as Grafana: host = 127.0.0.1:3306 or with Unix sockets: host = /var/run/mysqld/mysqld.sock.
        ##
        host: $__file{/etc/secrets/dashboardhost/host}
        ## The name of the Grafana database. Leave it set to grafana (default) or some other name.
        ## <ATTENTION> - You must create the database manually before deploying. If you are using the default database
        ## name, you must create a database named "grafana". The database user, if not a superuser, will require USAGE
        ## and CREATE privileges on the "public" schema and SELECT, INSERT, UPDATE, and DELETE privileges on all tables
        ## in the "public" schema. 
        # name: "database-name"
        ## Use either URL or the other fields above to configure the database.
        ## url: postgres://dashboardhost:abc123@dashboardhostpostgrescluster-primary.systemlink-nic2.svc:5432/grafana
        ## For PostgresSQL, use either disable, require or verify-full. For MySQL, use either true, false, or skip-verify.
        ##
        ssl_mode: "require"
        ## The path to the CA certificate to use. On many Linux systems, certs can be found in /etc/ssl/certs.
        # <ATTENTION> - The filename here must mach database.postgresCertificateFileName.
        ##
        ca_cert_path: "/etc/ssl/certs/dashboardhost/postgres-tls-certificate.pem"

      ## Plugin configuration
      ## Uncomment to set Grafana plugin configuration.
      ##
      # plugins:
        ## Enter a comma-separated list of plugin identifiers to identify plugins that are allowed to be loaded even if they lack a valid signature.
        ## <ATTENTION> - Uncomment to set the list of unsigned plugins to load. This will override the plugins included by default so ensure you maintain
        ## the list of plugins listed in the default values file.
        ##
        # allow_loading_unsigned_plugins: ni-slnotebook-datasource,ni-sldataframe-datasource,ni-plotly-panel

## Grafana provisioning
##
grafanaprovisioning:
  grafanaDashboardsProvisioning:
    ## Dashboards listed are enumerated over the dashboard-configmap-template.yaml
    ## Each key under the "dashboards" item will be the name of a ConfigMap.
    ## It is recommended to add the "-dashboard" suffix to each key to avoid collision with other ConfigMaps
    ## Dashboard json files can be placed under "dashboards/[project-name]/[dashboard-file].json"
    ## By default, all "enabled" parameters are considered as true even if they are not mentioned. You can disable this by assigning "whitelistMode: true"
    ##
    whitelistMode: true
  grafanaDatasourcesProvisioning:
    ## Datasources listed are enumerated over the datasource-configmap-template.yaml
    ## Each key under the "datasources" item will be the name of a datasource in the ConfigMap.
    ## By default, all "enabled" parameters are considered as true even if they are not mentioned. You can disable this by assigning "whitelistMode: true"
    ##
    whitelistMode: true
    projects:
      ## Datasources must be grouped under projects
      ## There is an optional "enabled" parameter that defaults to true which can be used to toggle the project/dashboard on or off.
      # some-project:
      #   enabled: false | true
      #   datasources:
      #     some-datasource:
      #       enabled: false | true
      #       name: Some Datasource
      #       type: some-datasource
      #       ## Each datasource is then copied as written into the template. You can follow the format here to define an entry: https://github.com/grafana/helm-charts/blob/307aae1ba29039c4c80581d457299c0a126b55c1/charts/grafana/values.yaml#L462
      #       ## The main difference is that instead of a list of datasources we use a key-value pair. The values for it can be defined the same way as in the grafana docs.
      ## <ATTENTION> - Uncomment to override the enabled plugins. This will override the plugins enabled by default so ensure you enable the plugins listed in the default values file.
      # systemlink:
      #   enabled: true
      #   datasources:
      #     ni-slnotebook-datasource:
      #       enabled: true
      #     ni-sldataframe-datasource:
      #       enabled: true

## Configuration for the Data Frame service.
##
dataframeservice:
  ## Ingress configuration
  ##
  ingress:
    ## Increase the maximum HTTP request body size from the nginx default. Only applies if an nginx
    ## ingress controller is used. Should be set to the same size as requestBodySizeLimitMegabytes.
    ##
    annotations:
      nginx.ingress.kubernetes.io/proxy-body-size: 256m

  ## <ATTENTION> - Configure ingestion appendable table limit.
  ##
  ingestion:
    ## The number of distinct tables that can be appended to before table creation will be blocked.
    ## To stay under this limit, set 'endOfData: true' on tables that don't need to be appended to anymore.
    ## For more information, visit ni.com/r/setendofdata.
    ##
    appendableTableLimit: 250

  ## Limits the body size for requests in megabytes. The ingress may also impose a request body size
  ## limit, which should be set to the same value.
  ##
  requestBodySizeLimitMegabytes: 256

  ## <ATTENTION> - Configure rate limiting. Limits are enforced per-replica. 
  ## Each replica of the dataframe service applies its own limit. 
  ## Considering load-balancing, the effective rate will be higher than the
  ## individual rates configured here.
  ##
  rateLimits:
    ## Configure rate limits for ingestion
    ##
    ingestion:
      ## Number of concurrent requests that a single replica can serve for ingesting data.
      ## Subsequent requests will be put in a queue.
      ##
      requestsLimit: 20
      ## Size of the queue for concurrent requests. If a request arrives to a pod with a full queue,
      ## the replica will return a 429 Error code.
      queueSize: 0

  ## Configure S3/MinIO access.
  ##
  s3:
    auth:
      ## Name of the secret containing the S3 or MinIO login credentials
      ##
      secretName: "nidataframe-s3-credentials"
    ## The name of the S3 or MinIO bucket that the service should connect to.
    ##
    bucket: &dataframeBucket "systemlink-dataframe"
    ## This should just be the name of the scheme, without the trailing ://.
    ##
    schemeName: "http"
    ## Set this value to connect to an external S3 instance.
    # <ATTENTION> To connect to an external S3 bucket, set the host here as well as the schemeName and port.
    ##
    host: ""
    ## Set this value to connect to an S3 instance which is internal to the cluster. Ignored if host is set.
    ##
    service: *minioServiceName
    ## S3 port number.
    # <ATTENTION> This must be overridden if not using the SLE MinIO instance.
    ##
    port: *minioPort
  ## Configure Dremio access
  ##
  sldremio:
    ## Uncomment this section to adjust the resource requests for the Dremio executor and coordinator.
    ## Refer to Dremio documentation at https://docs.dremio.com/software/deployment/system-requirements/#server-or-instance-hardware
    ## for a description of the recommended minimum values.
    # <ATTENTION> - These are the recommended values for the executor and the coordinator where the coordinator
    ## can act as an executor. Although, 16 CPU cores and 128GB of RAM are recommended, 1 CPU core and 8GB of RAM
    ## are left for the OS.
    ##
    # coordinator:
    #   cpu: 15
    #   memory: 122800
    #   volumeSize: 128Gi
    # executor:
    #   count: 4
    #   cpu: 15
    #   memory: 122800
    #   volumeSize: 128Gi
    ## CPU and memory allocated to each zookeeper pod, expressed in CPU cores and MB respectively.
    ## Count should correspond with the number of nodes that received the "high.mem" label.
    # zookeeper:
    #   cpu: 0.5
    #   memory: 1024
    #   count: 3
    ## Nodes to select for Dremio pods.
    # nodeSelector:
    #   high.mem: "true"
    auth:
      ## Name of the secret containing the Dremio login credentials
      ##
      secretName: "nidataframe-dremio-credentials"
      ## The name of the key in the above secret whose value contains the Dremio username
      ##
      usernameKey: "username"
      ## The name of the key in the above secret whose value contains the Dremio password
      ##
      passwordKey: "password"
    distStorage:
      # Dremio distributed storage configuration. This must be configured for the service to perform acceptably.
      # See https://github.com/dremio/dremio-cloud-tools/blob/master/charts/dremio_v2/docs/Values-Reference.md#distributed-storage-values
      # <ATTENTION>: This only partially configures distributed storage. Credentials must be configured in systemlink-secrets.yaml under
      # sldremio.distStorage.aws.credentials.
      type: "aws" # <ATTENTION> - change this if not using Amazon S3 or an equivalent, like MinIO
      aws:
        bucketName: *dataframeBucket
        path: "/dremio/distStorage"
        authentication: "accessKeySecret"
        # extraProperties are only necessary if not using Amazon S3
        # <ATTENTION>: If using Amazon S3, comment out extraProperties. If using MinIO or an equivalent, set fs.s3a.endpoint
        # to the FQDN of MinIO or the equivalent service.
        extraProperties: |
          <property>
            <name>fs.s3a.endpoint</name>
            <value><ATTENTION> - set to the FQDN of the S3 endpoint, including the port, but without an HTTP or HTTPS prefix. Example: {svc-name}.{namespace}.svc.cluster.local:9000</value>
          </property>
          <property>
            <name>fs.s3a.path.style.access</name>
            <description>Value has to be set to true.</description>
            <value>true</value>
          </property>
          <property>
            <name>dremio.s3.compat</name>
            <description>Value has to be set to true.</description>
            <value>true</value>
          </property>
          <property>
            <name>fs.s3a.connection.ssl.enabled</name>
            <description>Value can either be true or false, set to true to use SSL with a secure Minio server.</description>
            <value>false</value>
          </property>

  ## Configure Kafka access
  ##
  kafkaconnect:
    spec:
      template:
        pod:
          ## Configure image pull secrets for the Kafka container.
          ##
          imagePullSecrets: *pullSecrets

  ## Configure Kafka UI
  ##
  kafka-ui:
    imagePullSecrets:
      - name: *niPullSecret

## Salt configuration.
##
saltmaster:
  ## Configure the TCP ingress for the Salt API.
  ##
  serviceTCP:
    annotations:
      # <ATTENTION> - Set to the name of a MetalLB address group configured to allow TCP access to the Salt API.
      metallb.universe.tf/address-pool: "systemlink"

## File upload configuration.
##
fileingestion:
  ## Ingress configuration
  ##
  ingress:
    ## Override the default upload limit for an nginx ingress controller.
    ##
    annotations:
      nginx.ingress.kubernetes.io/proxy-body-size: 2000m
      nginx.ingress.kubernetes.io/proxy-request-buffering: "off"
      nginx.ingress.kubernetes.io/proxy-buffering: "off"

  ## Configure S3/MinIO access.
  ##
  s3:
    ## Secret name for S3 credentials.
    ##
    secretName: "fileingestion-s3-credentials"
    ## The name of the S3 or MinIO bucket that the service should connect to.
    ##
    bucket: "systemlink-file-ingestion"
    ## Set this to true to limit each user to a maximum of 1Gb of file storage.
    ##
    storageLimitsEnabled: false
    ## S3 connection scheme.
    ##
    scheme: "http://"
    ## Set this value to connect to an external S3 instance.
    # <ATTENTION> To connect to an external S3 bucket, set the host here as well as the scheme and port.
    ##
    host: ""
    ## Set this value to connect to an S3 instance which is internal to the cluster. Ignored if host is set.
    ##
    service: *minioServiceName
    ## S3 Port
    # <ATTENTION> This must be overridden if not using the SLE MinIO instance.
    ##
    port: *minioPort
   ## Configure rate limiting. Limits are enforced per-user.  Each replica of the file ingestion service
   ## applies its own per-user limit. With load-balancing, the effective rate will be higher than the
   ## individual rates configured here.
  ##
  rateLimits:
    ## Upload file
    ##
    upload: 3
  # Configure the maximum accepted size of uploaded files, expressed in gigabytes.
  uploadLimitGB: 10

## Configuration for JupyterHub.
##
sl-jupyterhub:
  jupyterhub:
    imagePullSecrets: *pullSecrets
    hub:
      extraEnv:
        ## Setting this to "true" will enable the legacy implementation of Jupyter usernames.
        JUPYTER_USERNAME_AS_SYSTEMLINK_USER_ID: "false"
    singleuser:
      networkPolicy:
        egressAllowRules:
          # <ATTENTION> - Set to true to enable JupyterHub user pods to establish outbound connections to private IP addresses.
          privateIPs: false
      # JupyterHub storage configuration.    
      storage:
        ## PVC Storage Request for every JupyterHub userpod.
        # <ATTENTION> - Adjust this size to match the needs of your application.
        capacity: 1Gi
      
  ## Ingress configuration.
  ##
  ingress:
    ## Override the default upload limit for an nginx ingress controller.
    ##
    annotations:
      nginx.ingress.kubernetes.io/proxy-body-size: 100m

## Configuration for Argo Workflows.
##
argoworkflows:
  argo-workflows:
    images:
      pullSecrets: *pullSecrets
    controller:
      image:
        registry: *imageRegistryRef
      ## Maximum number of workflows that can run in parallel.
      ##
      parallelism: 300
      workflowDefaults:
        spec:
          ttlStrategy:
            ## How long should the workflow live in case of a failure.
            ##
            secondsAfterFailure: 600
            ## How long should the workflow live in case of a success.
            ##
            secondsAfterSuccess: 0
    executor:
      image:
        registry: *imageRegistryRef
    server:
      image:
        registry: *imageRegistryRef

## Configuration for Notebook Execution service.
##
nbexecservice:
  argo:
    ## Configure S3/MinIO access.
    ##
    artifactRepository:
      s3:
        ## The name of the S3 or MinIO bucket that the service should connect to.
        ##
        bucket: "systemlink-notebook-execution"
        ## Set this value to connect to an external S3 instance.
        # <ATTENTION> To connect to an external S3 bucket, set the host here as well as the insecure and port.
        ##
        host: ""
        ## Set this value to connect to an S3 instance which is internal to the cluster. Ignored if host is set.
        ##
        service: *minioServiceName
        ## S3 Port
        # <ATTENTION> This must be overridden if not using the SLE MinIO instance.
        ##
        port: *minioPort
        ## Set this value to true for an http (insecure) connection, or false for an https (secure) connection.
        ##
        insecure: true
        ## Secret PATH containing the credentials.
        accessKeySecret:
          name: "notebookexecution-s3-credentials"
          key: "username"
        secretKeySecret:
          name: "notebookexecution-s3-credentials"
          key: "password"

## Configuration for licensing
##
license:
  ## Persistent Volume configuration
  ##
  persistentVolume:
    # <ATTENTION> Set this value to the name of the persistent storage class that supports ReadWriteMany access.
    ## Leave this value as empty, to use the default storage class, if it already supports ReadWriteMany access.
    ##
    storageClassName: ""

## Configuration for event-based routine triggering.
##
routineeventtrigger:
  ## Number of routine event trigger replicas to deploy.
  ##
  # replicaCount: 5

## Configuration for time-based routine triggering.
##
routinescheduletrigger:
  ## Number of routine schedule trigger replicas to deploy.
  ##
  # replicaCount: 2

## Configuration for SMTP
smtp:
  smtpServer:
    # <ATTENTION> Set the following values to enable email notifications.
    ## Exclude this section to leave the SMTP service disabled,
    ##
    ## Required - The SMTP server host name.
    ##
    # host: "example"
    ## Required - The SMTP server port.
    ##
    # port: 1025
    ## Required - The from address used in notification emails.
    ##
    # fromAddress: "noreply@example.com"
    ## Optional - Whether SSL should be enabled.
    ##
    # enableSsl: true
    ## Optional - Whether credentials are required to connect to the SMTP server. If this is true, the smtp-server-credentials secret must exist.
    ##
    # requireAuthentication: true
    ## Optional - The timeout, in seconds, when communicating with the SMTP server.
    ##
    # timeoutSeconds: 100
